DQN erkundet gut, aber es gab Probleme mit den Kanten weshalb wie die Runden Objekte eingefügt haben



DEFAULT_USER_DEBUG_GUI = False
DEFAULT_ADVANCED_STATUS_PLOT = False

DEFAULT_USE_PRETRAINED_MODEL = False
# DEFAULT_PRETRAINED_MODEL_PATH = '/home/florian/Documents/gym-pybullet-drones/results/durchgelaufen-DQN/final_model.zip'
# DEFAULT_PRETRAINED_MODEL_PATH = "/home/alex/Documents/RKIM/Semester_1/F&E_1/Dronnenrennen_Group/gym-pybullet-drones/results/save-03.07.2025_02.23.46/best_model.zip"
DEFAULT_PRETRAINED_MODEL_PATH = (
"/home/florian/Documents/gym-pybullet-drones/gym_pybullet_drones/Auswertung_der_Modelle_Archieve/M6_R6_O5_A3_TR1_T1_20250407-013856/save-04.07.2025_01.38.56/best_model.zip"
)

Ziel_Training_TIME_In_Simulation = 24 * 60 * 60 # 5 Stunden
DEFAULT_EVAL_FREQ = 5 * 1e4
DEFAULT_EVAL_EPISODES = 1

DEFAULT_TRAIN_TIMESTEPS = 8 * 1e5 # nach 100000 Steps sollten schon mehrbahre Erkenntnisse da sein
DEFAULT_TARGET_REWARD = 99999
DEFAULT_NUMBER_LAST_ACTIONS = 20

# file_path = "gym_pybullet_drones/examples/MAZE_TRAINING/Maze_init_target.yaml"
file_path = os.path.join(os.path.dirname(__file__), "Maze_init_target.yaml")

def loadyaml_(file_path):
with open(file_path, "r") as file:
return yaml.safe_load(file)

Target_Start_Values = loadyaml_(file_path)

# Initialisieren Sie das Dictionary, um die Werte zu speichern
INIT_XYZS = {}
DEFAULT_TARGET_POSITION = {}

DEFAULT_ALTITUDE = 0.5

# Iterieren Sie über die Maps und speichern Sie die Werte im Dictionary
for map_name, map_values in Target_Start_Values.items():
INIT_XYZS[map_name] = (map_values["initial_xyzs"],)
DEFAULT_TARGET_POSITION[map_name] = (map_values["target_position"],)
# INIT_RPYS = map_values['initial_rpys']

DEFAULT_RECORD_VIDEO = False
DEFAULT_OUTPUT_FOLDER = "results"
DEFAULT_OUTPUT_FOLDER = "results"
DEFAULT_COLAB = False
DEFAULT_PYB_FREQ = 100
DEFAULT_CTRL_FREQ = 50
DEFAULT_REWARD_AND_ACTION_CHANGE_FREQ = 2 # mit 5hz fliegt die Drohne noch zu oft an die Wand, ohne das das Pushback aktiv werden kann (mit Drehung aktiv) -> 10 HZ
DEFAULT_EPISODE_LEN_SEC = 5 * 60 # 15 * 60
DEFAULT_DRONE_MODEL = DroneModel("cf2x")
DEFAULT_PUSHBACK_ACTIVE = False

DEFAULT_EVAL_FREQ = 5 * 1e4
DEFAULT_EVAL_EPISODES = 1

DEFAULT_TRAIN_TIMESTEPS = Ziel_Training_TIME_In_Simulation * DEFAULT_REWARD_AND_ACTION_CHANGE_FREQ # nach 100000 Steps sollten schon mehrbahre Erkenntnisse da sein
DEFAULT_TARGET_REWARD = 99999999999999

DEFAULF_NUMBER_LAST_ACTIONS = 80

DEFAULT_OBS = ObservationType("kin") # 'kin' or 'rgb'
DEFAULT_ACT = ActionType("vel") # 'rpm' or 'pid' or 'vel' or 'one_d_rpm' or 'one_d_pid'
DEFAULT_AGENTS = 1
DEFAULT_MA = False

DEFAULT_DASH_ACTIVE = False

DEFAULT_Multiplier_Collision_Penalty = 2

DEFAULT_VelocityScale = 0.5

# Bei wie viel Prozent der Fläche einen Print ausgeben
DEFAULT_Procent_Step = 0.01
DEFAULT_REWARD_FOR_NEW_FIELD = 4
DEFAULT_Punishment_for_Step = -0.5
# 5 bedeutet eine 5x5 Matrix
DEFAULT_explore_Matrix_Size = 5
DEFAULT_Maze_number = 21
# nach wie vielen Schritten wird ein neues maze gewählt
DEFAULT_New_Maze_number = 10
DEFAULT_New_Position_number = 1

DEFAULT_collision_penalty_terminated = -100 # mit -10 Trainiert SAC gut, bleibt aber noch ca. 50 mal an der Wand hängen--
DEFAULT_Terminated_Wall_Distance = 0.15 # worst case betrachtung; wenn Drohe im 45 Grad winkel auf die Wand schaut muss dieser mit cos(45) verrechnet werden --> Distanz: 0,25 -> Worstcase-Distanz = 0,18 ; 0,3 -> 0,21; 0,35 --> 0,25
DEFAULT_no_collision_reward = 1 # nur bei R5 aktiv! Ist das Zuckerbrot für den Abstand zur Wand

# R7 - Negative Reward Map Settings
DEFAULT_Punishment_for_Walls = 8
DEFAULT_Influence_of_Walls = 4


